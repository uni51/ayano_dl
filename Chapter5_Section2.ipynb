{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# やさしく学ぶディープラーニングがわかる数学の基本 \n",
    "# Chapter5 ニューラルネットワークを実装してみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 アスペクト比判定ニューラルネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 学習データの数\n",
    "N = 1000\n",
    "\n",
    "# （学習に再現性を持たせるためにシードを固定しています。本来は不要です）\n",
    "np.random.seed(1)\n",
    "\n",
    "# 適当な学習データと正解ラベルを生成\n",
    "TX = (np.random.rand(N, 2) * 1000).astype(np.int32) + 1\n",
    "TY = (TX.min(axis=1) / TX.max(axis=1) <= 0.2).astype(np.int)[np.newaxis].T #  行方向がaxis=0、列方向がaxis=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 標準化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習データの平均を0、分散を1に揃えることで、パラメータの収束の速度を上げることができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均と標準偏差を計算\n",
    "MU = TX.mean(axis=0) # 平均\n",
    "SIGMA = TX.std(axis=0) # 標準偏差\n",
    "\n",
    "# 標準化\n",
    "def standardize(X):\n",
    "        return (X - MU) / SIGMA\n",
    "\n",
    "TX = standardize(TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 Step1. ニューラルネットワークの構造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重みとバイアス\n",
    "W1 = np.random.randn(2, 2) # 第1層重み（2x2行列）\n",
    "W2 = np.random.randn(2, 2) # 第2層重み（2x2行列）\n",
    "W3 = np.random.randn(1, 2) # 第3層重み（1x2行列）\n",
    "b1 = np.random.randn(2) # 第1層バイアス（2x1行列）\n",
    "b2 = np.random.randn(2) # 第2層バイアス（2x1行列）\n",
    "b3 = np.random.randn(2) # 第3層バイアス（1x1行列）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 Step2. 順伝播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シグモイド関数\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 順伝播\n",
    "def forward(X0):\n",
    "    Z1 = np.dot(X0, W1.T) + b1\n",
    "    X1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(X1, W2.T) + b2\n",
    "    X2 = sigmoid(Z2)\n",
    "    Z3 = np.dot(X2, W3.T) + b3\n",
    "    X3 = sigmoid(Z3)\n",
    "\n",
    "    return Z1, X1, Z2, X2, Z3, X3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 Step3. 逆伝播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆伝播の処理に必要なのは、シグモイド関数の微分、出力層のデルタ、隠れ層のデルタの3つ。それぞれ順番に実装していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シグモイド関数の微分\n",
    "def dsigmoid(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力層のデルタ\n",
    "def delta_output(Z, Y):\n",
    "    return (sigmoid(Z) - Y) * dsigmoid(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隠れ層のデルタ\n",
    "def delta_hidden(Z, D, W):\n",
    "    return dsigmoid(Z) * np.dot(D, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 逆伝播\n",
    "def backward(Y, Z3, Z2, Z1):\n",
    "    D3 = delta_output(Z3, Y)\n",
    "    D2 = delta_hidden(Z2, D3, W3)\n",
    "    D1 = delta_hidden(Z1, D2, W2)\n",
    "\n",
    "    return D3, D2, D1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 Step4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率\n",
    "ETA = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的関数の重みでの微分\n",
    "def dweight(D, X):\n",
    "    return np.dot(D.T, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的関数のバイアスでの微分\n",
    "def dbias(D):\n",
    "    return D.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータの更新\n",
    "def update_parameters(D3, X2, D2, X1, D1, X0):\n",
    "    global W3, W2, W1, b3, b2, b1\n",
    "\n",
    "    W3 = W3 - ETA * dweight(D3, X2)\n",
    "    W2 = W2 - ETA * dweight(D2, X1)\n",
    "    W1 = W1 - ETA * dweight(D1, X0)\n",
    "\n",
    "    b3 = b3 - ETA * dbias(D3)\n",
    "    b2 = b2 - ETA * dbias(D2)\n",
    "    b1 = b1 - ETA * dbias(D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習\n",
    "def train(X, Y):\n",
    "    # 順伝播\n",
    "    Z1, X1, Z2, X2, Z3, X3 = forward(X)\n",
    "\n",
    "    # 逆伝播\n",
    "    D3, D2, D1 = backward(Y, Z3, Z2, Z1)\n",
    "\n",
    "    # パラメータの更新\n",
    "    update_parameters(D3, X2, D2, X1, D1, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繰り返し回数\n",
    "EPOCH = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測\n",
    "def predict(X):\n",
    "    return forward(X)[-1]\n",
    "\n",
    "# 目的関数\n",
    "def E(Y, X):\n",
    "    return 0.5 * ((Y - predict(X)) ** 2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section2 Step５. ミニバッチ法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (100,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-876380468001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mY\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mTY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# ログを残す\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-b3256dbc6183>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 逆伝播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mD3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# パラメータの更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-5e561e4c4027>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(Y, Z3, Z2, Z1)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mD3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mD2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mD1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cf4885a77db9>\u001b[0m in \u001b[0;36mdelta_hidden\u001b[0;34m(Z, D, W)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 隠れ層のデルタ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdelta_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdsigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (100,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# ミニバッチ数\n",
    "BATCH = 100\n",
    "\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    # ミニバッチ学習用にランダムなインデックスを取得\n",
    "    p = np.random.permutation(len(TX))\n",
    "\n",
    "    # ミニバッチの数分だけデータを取り出して学習\n",
    "    for i in range(math.ceil(len(TX) / BATCH)):\n",
    "        indice = p[i*BATCH:(i+1)*BATCH]\n",
    "        X0 = TX[indice]\n",
    "        Y  = TY[indice]\n",
    "\n",
    "        train(X0, Y)\n",
    "\n",
    "    # ログを残す\n",
    "    if epoch % 1000 == 0:\n",
    "        log = '誤差 = {:8.4f} ({:5d}エポック目)'\n",
    "        print(log.format(E(TY, TX), epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（上記のセルでの実行だとエラーになるので、）ここまでのロジックをまとめて実行する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "誤差 =  69.7705 ( 1000エポック目)\n",
      "誤差 =  55.0522 ( 2000エポック目)\n",
      "誤差 =  44.4299 ( 3000エポック目)\n",
      "誤差 =  24.9272 ( 4000エポック目)\n",
      "誤差 =  15.0983 ( 5000エポック目)\n",
      "誤差 =  11.5577 ( 6000エポック目)\n",
      "誤差 =   9.6375 ( 7000エポック目)\n",
      "誤差 =   8.4023 ( 8000エポック目)\n",
      "誤差 =   7.5279 ( 9000エポック目)\n",
      "誤差 =   6.8681 (10000エポック目)\n",
      "誤差 =   6.3472 (11000エポック目)\n",
      "誤差 =   5.9225 (12000エポック目)\n",
      "誤差 =   5.5672 (13000エポック目)\n",
      "誤差 =   5.2649 (14000エポック目)\n",
      "誤差 =   5.0039 (15000エポック目)\n",
      "誤差 =   4.7761 (16000エポック目)\n",
      "誤差 =   4.5755 (17000エポック目)\n",
      "誤差 =   4.3975 (18000エポック目)\n",
      "誤差 =   4.2384 (19000エポック目)\n",
      "誤差 =   4.0956 (20000エポック目)\n",
      "誤差 =   3.9667 (21000エポック目)\n",
      "誤差 =   3.8496 (22000エポック目)\n",
      "誤差 =   3.7430 (23000エポック目)\n",
      "誤差 =   3.6453 (24000エポック目)\n",
      "誤差 =   3.5555 (25000エポック目)\n",
      "誤差 =   3.4725 (26000エポック目)\n",
      "誤差 =   3.3955 (27000エポック目)\n",
      "誤差 =   3.3238 (28000エポック目)\n",
      "誤差 =   3.2566 (29000エポック目)\n",
      "誤差 =   3.1936 (30000エポック目)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 学習データの数\n",
    "N = 1000\n",
    "\n",
    "# (学習に再現性をもたせるために シードを固定しています。本来は不要です)\n",
    "np.random.seed(1)\n",
    "\n",
    "# 適当な学習データと正解ラベルを生成\n",
    "TX = (np.random.rand(N, 2) * 1000).astype(np.int32) + 1\n",
    "TY = (TX.min(axis=1) / TX.max(axis=1) <= 0.2).astype(np.int)[np.newaxis].T\n",
    "\n",
    "# 平均と標準偏差を計算\n",
    "MU = TX.mean(axis=0)\n",
    "SIGMA = TX.std(axis=0)\n",
    "\n",
    "# 標準化\n",
    "def standardize(X):\n",
    "    return (X - MU) / SIGMA\n",
    "\n",
    "TX = standardize(TX)\n",
    "\n",
    "# 重みとバイアス\n",
    "W1 = np.random.randn(2, 2) # 第1層重み\n",
    "W2 = np.random.randn(2, 2) # 第2層重み\n",
    "W3 = np.random.randn(1, 2) # 第3層重み\n",
    "b1 = np.random.randn(2)    # 第1層バイアス\n",
    "b2 = np.random.randn(2)    # 第2層バイアス\n",
    "b3 = np.random.randn(1)    # 第3層バイアス\n",
    "\n",
    "# シグモイド関数\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# 順伝播\n",
    "def forward(X0):\n",
    "    Z1 = np.dot(X0, W1.T) + b1\n",
    "    X1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(X1, W2.T) + b2\n",
    "    X2 = sigmoid(Z2)\n",
    "    Z3 = np.dot(X2, W3.T) + b3\n",
    "    X3 = sigmoid(Z3)\n",
    "\n",
    "    return Z1, X1, Z2, X2, Z3, X3\n",
    "\n",
    "# シグモイド関数の微分\n",
    "def dsigmoid(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "# 出力層のデルタ\n",
    "def delta_output(Z, Y):\n",
    "    return (sigmoid(Z) - Y) * dsigmoid(Z)\n",
    "\n",
    "# 隠れ層のデルタ\n",
    "def delta_hidden(Z, D, W):\n",
    "    return dsigmoid(Z) * np.dot(D, W)\n",
    "\n",
    "# 逆伝播\n",
    "def backward(Y, Z3, Z2, Z1):\n",
    "    D3 = delta_output(Z3, Y)\n",
    "    D2 = delta_hidden(Z2, D3, W3)\n",
    "    D1 = delta_hidden(Z1, D2, W2)\n",
    "\n",
    "    return D3, D2, D1\n",
    "\n",
    "# 学習率\n",
    "ETA = 0.001\n",
    "\n",
    "# 目的関数の重みでの微分\n",
    "def dweight(D, X):\n",
    "    return np.dot(D.T, X)\n",
    "\n",
    "# 目的関数のバイアスでの微分\n",
    "def dbias(D):\n",
    "    return D.sum(axis=0)\n",
    "\n",
    "# パラメータの更新\n",
    "def update_parameters(D3, X2, D2, X1, D1, X0):\n",
    "    global W3, W2, W1, b3, b2, b1\n",
    "\n",
    "    W3 = W3 - ETA * dweight(D3, X2)\n",
    "    W2 = W2 - ETA * dweight(D2, X1)\n",
    "    W1 = W1 - ETA * dweight(D1, X0)\n",
    "\n",
    "    b3 = b3 - ETA * dbias(D3)\n",
    "    b2 = b2 - ETA * dbias(D2)\n",
    "    b1 = b1 - ETA * dbias(D1)\n",
    "\n",
    "# 学習\n",
    "def train(X, Y):\n",
    "    # 順伝播\n",
    "    Z1, X1, Z2, X2, Z3, X3 = forward(X)\n",
    "\n",
    "    # 逆伝播\n",
    "    D3, D2, D1 = backward(Y, Z3, Z2, Z1)\n",
    "\n",
    "    # パラメータの更新\n",
    "    update_parameters(D3, X2, D2, X1, D1, X)\n",
    "\n",
    "# 繰り返し回数\n",
    "EPOCH = 30000\n",
    "\n",
    "# 予測\n",
    "def predict(X):\n",
    "    return forward(X)[-1]\n",
    "\n",
    "# 目的関数\n",
    "def E(Y, X):\n",
    "    return 0.5 * ((Y - predict(X)) ** 2).sum()\n",
    "\n",
    "# ミニバッチ数\n",
    "BATCH = 100\n",
    "\n",
    "for epoch in range(1, EPOCH + 1):\n",
    "    # ミニバッチ学習用にランダムなインデックスを取得\n",
    "    p = np.random.permutation(len(TX))\n",
    "\n",
    "    # ミニバッチの数分だけデータを取り出して学習\n",
    "    for i in range(math.ceil(len(TX) / BATCH)):\n",
    "        indice = p[i*BATCH:(i+1)*BATCH]\n",
    "        X0 = TX[indice]\n",
    "        Y  = TY[indice]\n",
    "\n",
    "        train(X0, Y)\n",
    "\n",
    "    # ログを残す\n",
    "    if epoch % 1000 == 0:\n",
    "        log = '誤差 = {:8.4f} ({:5d}エポック目)'\n",
    "        print(log.format(E(TY, TX), epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00097628],\n",
       "       [0.82436398],\n",
       "       [0.94022858],\n",
       "       [0.00173001]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX = standardize([\n",
    "    [100, 100], # 正方形。細長くないはず\n",
    "    [100, 10], # 細長いはず \n",
    "    [10, 100], # これも細長いはず \n",
    "    [80, 100], # これは細長くないはず            \n",
    "])\n",
    "\n",
    "predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精度: 98.4%\n"
     ]
    }
   ],
   "source": [
    "# 分類器\n",
    "def classify(X):\n",
    "    return (predict(X) > 0.8).astype(np.int)\n",
    "\n",
    "# テストデータ生成\n",
    "TEST_N = 1000\n",
    "testX = (np.random.rand(TEST_N, 2) * 1000).astype(np.int32) + 1\n",
    "testY = (testX.min(axis=1) / testX.max(axis=1) <= 0.2).astype(np.int)[np.newaxis].T\n",
    "\n",
    "# 精度計算\n",
    "accuracy = (classify(standardize(testX)) == testY).sum() / TEST_N\n",
    "print('精度: {}%'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
